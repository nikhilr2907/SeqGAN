{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXRz10YwNjcq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GenLoader Code\n",
        "#Generator loader Code\n",
        "\n",
        "class gen_Data_loader():\n",
        "  def __init__(self,batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.token_stream = []\n",
        "\n",
        "  def create_batches(self,data_file):\n",
        "    self.token_stream = []\n",
        "    with open(data_file, \"r\") as f:\n",
        "      for line in f:\n",
        "        line = line.strip()\n",
        "        line = line.split()\n",
        "        parse_line = [int(x) for x in line]\n",
        "        if len(parse_line) == 20:\n",
        "\n",
        "          self.token_stream.append(parse_line)\n",
        "    self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
        "    self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
        "    self.sequence_batch = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
        "    self.pointer = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    ret = self.sequence_batch[self.pointer]\n",
        "    self.pointer = (self.pointer +1) % self.num_batch\n",
        "    return ret\n",
        "  def reset_pointer(self):\n",
        "    self.pointer = 0\n",
        "\n",
        "\n",
        "#Needed for pretraining the Generator\n"
      ],
      "metadata": {
        "id": "GY1O0ADQQS06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DisLoader Code\n",
        "#Discriminator loader Code\n",
        "\n",
        "\n",
        "class Dis_dataloader():\n",
        "  def __init__(self, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([])\n",
        "\n",
        "  def load_train_data(self, positive_file, negative_file):\n",
        "    positive_examples = []\n",
        "    negative_examples = []\n",
        "    with open(positive_file) as fin:\n",
        "      for line in fin:\n",
        "          line = line.strip()\n",
        "          line = line.split()\n",
        "          parse_line = [int(x) for x in line]\n",
        "          positive_examples.append(parse_line)\n",
        "    with open(negative_file)as fin:\n",
        "      for line in fin:\n",
        "          line = line.strip()\n",
        "          line = line.split()\n",
        "          parse_line = [int(x) for x in line]\n",
        "          if len(parse_line) == 20:\n",
        "            negative_examples.append(parse_line)\n",
        "          self.sentences = np.array(positive_examples + negative_examples)\n",
        "          positive_labels = [[0, 1] for _ in positive_examples]\n",
        "          negative_labels = [[1, 0] for _ in negative_examples]\n",
        "          self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
        "\n",
        "          # Shuffle the data\n",
        "          shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
        "          self.sentences = self.sentences[shuffle_indices]\n",
        "          self.labels = self.labels[shuffle_indices]\n",
        "\n",
        "          # Split batches\n",
        "          self.num_batch = int(len(self.labels) / self.batch_size)\n",
        "          self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
        "          self.labels = self.labels[:self.num_batch * self.batch_size]\n",
        "          self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
        "          self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
        "\n",
        "          self.pointer = 0\n",
        "  def next_batch(self):\n",
        "      ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
        "      self.pointer = (self.pointer + 1) % self.num_batch\n",
        "      return ret\n",
        "\n",
        "  def reset_pointer(self):\n",
        "      self.pointer = 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ob7LZiPPXJS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Discriminator Code\n",
        "#Discriminator Code\n",
        "\n",
        "\n",
        "\n",
        "#Linear layer code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def linear(input__, output_size, scope=None):\n",
        "  shape = input__.get_shape().as_list()\n",
        "  if len(shape) != 2:\n",
        "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
        "  if not shape[1]:\n",
        "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
        "  input_size = shape[1]\n",
        "\n",
        "\n",
        "  with tf.variable_scope(scope or \"SimpleLinear\"):\n",
        "    matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype= input__.dtype)\n",
        "    bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input__.dtype)\n",
        "  return tf.matmul(input__, tf.transpose(matrix)) + bias_term\n",
        "\n",
        "\n",
        "def highway(input__, size, num_layers=1, bias=-2.0, f = tf.nn.relu, scope=\"Highway\"):\n",
        "    with tf.variable_scope(scope):\n",
        "      for idx in range(num_layers):\n",
        "        g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
        "\n",
        "        t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
        "\n",
        "        output = t * g + (1. - t) * input_\n",
        "        input_ = output\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "#Coding the actual discriminator itself\n",
        "\n",
        "class Discriminator(object):\n",
        "    def __init__(self, sequence_length, data_size,l2_reg_lambda=0.0):\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # l2 regularization for overfitting correction\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        with tf.variable_scope(\"discriminator\"):\n",
        "\n",
        "\n",
        "\n",
        "            def conv_layer(inputs, filters, kernel_size, name):\n",
        "                return tf.layers.conv2d(\n",
        "                    inputs=inputs,\n",
        "                    filters=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding='SAME',\n",
        "                    activation=tf.nn.relu,\n",
        "                    name=name\n",
        "                )\n",
        "\n",
        "            # First convolutional layer\n",
        "            conv1 = conv_layer(self.input_x, filters=64, kernel_size=3, name=\"conv1\")\n",
        "\n",
        "            # Second convolutional layer\n",
        "            conv2 = conv_layer(conv1, filters=32, kernel_size=3, name=\"conv2\")\n",
        "\n",
        "            def dense_layer(inputs, units, name, activation=tf.nn.relu):\n",
        "                return tf.layers.dense(\n",
        "                    inputs=inputs,\n",
        "                    units=units,\n",
        "                    activation=activation,\n",
        "                    name=name\n",
        "                )\n",
        "\n",
        "            # Flatten the output from conv2\n",
        "            flattened = tf.layers.flatten(conv2)\n",
        "\n",
        "            # Highway layers\n",
        "            highway_output = highway(flattened, flattened.get_shape()[1].value, num_layers=2)\n",
        "\n",
        "            # Dropout\n",
        "            highway_output_dropout = tf.nn.dropout(highway_output, self.dropout_keep_prob)\n",
        "\n",
        "            # Six fully connected layers with ReLU activation and dropout\n",
        "            fc1 = dense_layer(highway_output_dropout, units=1024, name=\"fc1\")\n",
        "            fc1_dropout = tf.nn.dropout(fc1, self.dropout_keep_prob)\n",
        "            fc2 = dense_layer(fc1_dropout, units=512, name=\"fc2\")\n",
        "            fc2_dropout = tf.nn.dropout(fc2, self.dropout_keep_prob)\n",
        "            fc3 = dense_layer(fc2_dropout, units=256, name=\"fc3\")\n",
        "            fc3_dropout = tf.nn.dropout(fc3, self.dropout_keep_prob)\n",
        "            fc4 = dense_layer(fc3_dropout, units=128, name=\"fc4\")\n",
        "            fc4_dropout = tf.nn.dropout(fc4, self.dropout_keep_prob)\n",
        "            fc5 = dense_layer(fc4_dropout, units=64, name=\"fc5\")\n",
        "            fc5_dropout = tf.nn.dropout(fc5, self.dropout_keep_prob)\n",
        "            fc6 = dense_layer(fc5_dropout, units=32, name=\"fc6\")\n",
        "            fc6_dropout = tf.nn.dropout(fc6, self.dropout_keep_prob)\n",
        "\n",
        "            def conv_pool_layer(inputs, filters, kernel_size, pool_size, name):\n",
        "                with tf.name_scope(name):\n",
        "                    # Convolutional Layer\n",
        "                    conv = tf.layers.conv2d(\n",
        "                        inputs=inputs,\n",
        "                        filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        padding='SAME',\n",
        "                        activation=tf.nn.relu,\n",
        "                        name=f\"{name}_conv\"\n",
        "                    )\n",
        "\n",
        "                    # Pooling Layer\n",
        "                    pool = tf.layers.max_pooling2d(\n",
        "                        inputs=conv,\n",
        "                        pool_size=pool_size,\n",
        "                        strides=pool_size,\n",
        "                        name=f\"{name}_pool\"\n",
        "                    )\n",
        "\n",
        "                return pool\n",
        "\n",
        "            # Reshape fc6 for conv_pool_layer\n",
        "            fc6_reshaped = tf.expand_dims(tf.expand_dims(fc6_dropout, 1), 1)\n",
        "\n",
        "            # Creating a pooled convolutional layer\n",
        "            pooled_conv = conv_pool_layer(\n",
        "                inputs=fc6_reshaped,\n",
        "                filters=64,\n",
        "                kernel_size=[1, 1],\n",
        "                pool_size=[1, 1],\n",
        "                name=\"pooled_conv_1\"\n",
        "            )\n",
        "\n",
        "            # Final scores and predictions\n",
        "            with tf.name_scope(\"output\"):\n",
        "                self.scores = tf.layers.dense(tf.layers.flatten(pooled_conv), num_classes, name=\"scores\")\n",
        "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "            # Calculate mean cross-entropy loss\n",
        "            with tf.name_scope(\"loss\"):\n",
        "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "                self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "            # Accuracy\n",
        "            with tf.name_scope(\"accuracy\"):\n",
        "                correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "                self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MARQoXUnc6rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator Code"
      ],
      "metadata": {
        "id": "ju38NlP4Mg6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generator Code\n",
        "# Generator code\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
        "\n",
        "\n",
        "class Generator(object):\n",
        "  def __init__(self, num_data, batch_size, hidden_dim,sequence_length, start_token, learning_rate=0.01, reward_gamma=0.95):\n",
        "    self.num_data = num_data\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.sequence_length = sequence_length\n",
        "    self.start_token = start_token\n",
        "    self.learning_rate = learning_rate\n",
        "    self.reward_gamma = reward_gamma\n",
        "    self.g_params = []\n",
        "    self.d_params = []\n",
        "    self.temperature = 1.0\n",
        "    self.grad_clip = 5.0\n",
        "\n",
        "\n",
        "    self.expected_reward = tf.Variable(tf.zeroes([self.sequence_length]))\n",
        "\n",
        "    with tf.variable_scope(\"generator\"):\n",
        "      self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)\n",
        "      self.g_output_unit = self.create_output_unit(self.g_params)\n",
        "\n",
        "\n",
        "    self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.sequence_length])\n",
        "    self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.sequence_length])\n",
        "\n",
        "\n",
        "    self.processed_x = self.x\n",
        "\n",
        "\n",
        "    self.h0 = tf.zeroes([self.batch_size, self.hidden_dim])\n",
        "\n",
        "    self.h0 = tf.stack([self.h0, self.h0])\n",
        "\n",
        "    self.gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,\n",
        "                                         dynamic_size=False, infer_shape=True)\n",
        "\n",
        "    self.gen_x = tensor_array_ops.TensorArray(dtype= tf.int32, size=self.sequence_length, dynamic_size=False, infer_shape=True)\n",
        "\n",
        "    def _g_recurrence(i,x_t,h_tm1,gen_x):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "\n",
        "      next_output = self.g_output_unit(h_t) #produces actual data instead of logits, produces actual data instead of logits\n",
        "      gen_x = gen_x.write(i, next_output)\n",
        "      x_tp1 = next_output\n",
        "      return i+1,x_tp1, h_t, gen_x\n",
        "\n",
        "\n",
        "    initial_value = tf.random.normal([self.batch_size, 1])\n",
        "\n",
        "\n",
        "    _,_, _, self.gen_x = control_flow_ops.while_loop(\n",
        "        cond=lambda i,_1, _2, _3 : i < self.sequence_length,\n",
        "        body=_g_recurrence,\n",
        "        loop_vars=(tf.constant(0,dtype=tf.int32),initial_value, self.h0, self.gen_x))\n",
        "    self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
        "    self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
        "\n",
        "    # Supervised Pretraining for Generator\n",
        "\n",
        "    g_predictions = tensor_array_ops.TensorArray(dtype= tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True)\n",
        "    seq_array = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length)\n",
        "    ta_emb_x = seq_array.unstack(self.x)\n",
        "\n",
        "\n",
        "    def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "      o_t = self.g_output_unit(h_t)\n",
        "      g_predictions = g_predictions.write(i, o_t)\n",
        "      x_tp1 = ta_emb_x.read(i)\n",
        "      return i+1, x_tp1, h_t, g_predictions\n",
        "\n",
        "    _,_,_,self.g_predictions = control_flow_ops.while_loop(cond=lambda i,_1, _2, _3:i < self.sequence_length, body=_pretrain_recurrence, loop_vars=(tf.constant(0,dtype=tf.int32), self.x[:,0,:], self.h0, g_predictions))\n",
        "\n",
        "    self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1,0,2])\n",
        "    self.pretrain_loss = -tf.reduce_mean(\n",
        "        tf.square(self.g_predictions - self.x)\n",
        "    )\n",
        "    pretrain_opt = self.g_optimizer(self.learning_rate)\n",
        "    self.pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(self.pretrain_loss, self.g_params), self.grad_clip)\n",
        "    self.pretrain_updates = pretrain_opt.apply_gradients(zip(self.pretrain_grad, self.g_params))\n",
        "    #######################################################################################################\n",
        "    #  Unsupervised Pretraining\n",
        "    #######################################################################################################\n",
        "    #g_predictions_reshaped = tf.reshape(self.g_predictions, [-1])\n",
        "    #x_reshaped = tf.reshape(self.x, [-1])\n",
        "\n",
        "    # Calculate squared error\n",
        "    #squared_error = tf.square(g_predictions_reshaped - x_reshaped)\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    #mse = tf.reduce_mean(squared_error)\n",
        "    #self.g_loss = tf.reduce_sum(mse * tf.reshape(self.rewards, [-1]))\n",
        "    #g_opt = self.g_optimizer(self.learning_rate)\n",
        "\n",
        "    self.g_grad, _ = tf.clip_by_global_norm(tf.gradients(self.g_loss, self.g_params), self.grad_clip)\n",
        "    self.g_updates = g_opt.apply_gradients(zip(self.g_grad, self.g_params))\n",
        "  def generate(self, sess):\n",
        "        outputs = sess.run(self.gen_x)\n",
        "        return outputs\n",
        "\n",
        "  def pretrain_step(self, sess, x):\n",
        "        outputs = sess.run([self.pretrain_updates, self.pretrain_loss], feed_dict={self.x: x})\n",
        "        return outputs\n",
        "\n",
        "  def init_matrix(self, shape):\n",
        "        return tf.random_normal(shape, stddev=0.1)\n",
        "\n",
        "  def init_vector(self, shape):\n",
        "        return tf.zeros(shape)\n",
        "\n",
        "  def create_recurrent_unit(self, params):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Ui = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bi = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wf = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uf = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bf = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wog = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uog = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bog = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "\n",
        "        self.Wc = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
        "        self.Uc = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
        "        self.bc = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
        "        params.extend([\n",
        "            self.Wi, self.Ui, self.bi,\n",
        "            self.Wf, self.Uf, self.bf,\n",
        "            self.Wog, self.Uog, self.bog,\n",
        "            self.Wc, self.Uc, self.bc])\n",
        "\n",
        "        def unit(self,x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "  def create_output_unit(self, params):\n",
        "        self.Wo = tf.Variable(self.init_matrix([self.hidden_dim, self.num_emb]))\n",
        "        self.bo = tf.Variable(self.init_matrix([self.num_emb]))\n",
        "        params.extend([self.Wo, self.bo])\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n",
        "  def g_optimizer(self, *args, **kwargs):\n",
        "        return tf.train.AdamOptimizer(*args, **kwargs)"
      ],
      "metadata": {
        "id": "Gbs_1N515zxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Oracle Model Code\n"
      ],
      "metadata": {
        "id": "4i-BsLlCT3jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to find a different model to make me the mock data here."
      ],
      "metadata": {
        "id": "lHU-ACqXODrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Rollout/ Reinforcement Learning Framework\n",
        "\n",
        "class ROLLOUT(object):\n",
        "  def __init__(self, lstm, update_rate):\n",
        "    self.lstm = lstm\n",
        "    self.update_rate = update_rate\n",
        "    self.sequence_length = self.lstm.sequence_length\n",
        "    self.start_token = tf.identity(self.lstm.start_token)\n",
        "    self.learning_rate = self.lstm.learning_rate\n",
        "    self.start_token = tf.identity(self.lstm.start_token)\n",
        "    self.g_recurrent_unit = self.create_recurrent_unit()\n",
        "    self.g_output_unit = self.create_output_unit()\n",
        "\n",
        "\n",
        "\n",
        "    self.x = tf.placeholder(tf.float32, shape=[self.lstm.batch_size, self.sequence_length])\n",
        "    self.given_num = tf.placeholder(tf.int32)\n",
        "\n",
        "    ta_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True)\n",
        "    ta_x = ta_x.unstack(self.x)\n",
        "\n",
        "\n",
        "    self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
        "    self.h0 = tf.stack([self.h0, self.h0])\n",
        "\n",
        "    gen_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length, dynamic_size=False, infer_shape=True)\n",
        "\n",
        "    def _g_recurrence_1(i, x_t, h_tm1, given_num, gen_x):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "      x_tp1 = ta_x.read(i)\n",
        "      gen_x = gen_x.write(i, ta_x.read(i))\n",
        "      return i+1, x_tp1, h_t, given_num, gen_x\n",
        "\n",
        "    def _g_recurrence_2(i, x_t, h_tm1, given_num, gen_x):\n",
        "      h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
        "      y_t = self.g_output_unit(h_t)\n",
        "\n",
        "      x_tp1 = y_t\n",
        "\n",
        "      gen_x = gen_x.write(i,y_t)  # indices, batch_size\n",
        "      return i + 1, x_tp1, h_t, given_num, gen_x\n",
        "\n",
        "    i, x_t, h_tm1, given_num, self.gen_x = control_flow_ops.while_loop(\n",
        "        cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
        "        body=_g_recurrence_1,\n",
        "        loop_vars=(tf.constant(0, dtype=tf.int32),ta_x.read(0) , self.h0, self.given_num, gen_x))\n",
        "\n",
        "\n",
        "    _,_,_,_, self.gen_x = control_flow_ops.while_loop(\n",
        "        cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
        "        body=_g_recurrence_2,\n",
        "        loop_vars=(i, x_t, h_tm1, self.given_num, self.gen_x))\n",
        "    self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
        "    self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
        "    ################################### MAIN REWARD FUNCTION AND MAIN PART OF THE REINFORCEMENT LEARNING FRAMEWORK\n",
        "  def get_reward(self, sess, input_x, rollout_num, discriminator):\n",
        "    rewards = []\n",
        "\n",
        "    for i in range(rollout_num):\n",
        "      for given_num in range(1, self.sequence_length):\n",
        "        feed = {self.x: input_x, self.given_num: given_num}\n",
        "        samples = sess.run(self.gen_x, feed_dict=feed)\n",
        "        feed = {discriminator.input_x: samples, discriminator.dropout_keep_prob:1.0}\n",
        "        ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n",
        "        ypred = np.array([item[1] for item in ypred_for_auc])\n",
        "        if i == 0:\n",
        "          rewards.append(ypred)\n",
        "        else:\n",
        "          rewards[given_num-1] += ypred\n",
        "\n",
        "      feed = {discriminator.input_x: input_x, discriminator.dropout_keep_prob:1.0}\n",
        "      ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n",
        "      ypred = np.array([item[1] for item in ypred_for_auc])\n",
        "      if i == 0:\n",
        "        rewards.append(ypred)\n",
        "      else:\n",
        "        rewards[self.sequence_length - 1] += ypred\n",
        "\n",
        "    rewards = np.transpose(np.array(rewards))/ (1.0 * rollout_num)\n",
        "    return rewards\n",
        "\n",
        "    def create_recurrent_unit(self):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = tf.identity(self.lstm.Wi)\n",
        "        self.Ui = tf.identity(self.lstm.Ui)\n",
        "        self.bi = tf.identity(self.lstm.bi)\n",
        "\n",
        "        self.Wf = tf.identity(self.lstm.Wf)\n",
        "        self.Uf = tf.identity(self.lstm.Uf)\n",
        "        self.bf = tf.identity(self.lstm.bf)\n",
        "\n",
        "        self.Wog = tf.identity(self.lstm.Wog)\n",
        "        self.Uog = tf.identity(self.lstm.Uog)\n",
        "        self.bog = tf.identity(self.lstm.bog)\n",
        "\n",
        "        self.Wc = tf.identity(self.lstm.Wc)\n",
        "        self.Uc = tf.identity(self.lstm.Uc)\n",
        "        self.bc = tf.identity(self.lstm.bc)\n",
        "\n",
        "        def unit(x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def update_recurrent_unit(self):\n",
        "        # Weights and Bias for input and hidden tensor\n",
        "        self.Wi = self.update_rate * self.Wi + (1 - self.update_rate) * tf.identity(self.lstm.Wi)\n",
        "        self.Ui = self.update_rate * self.Ui + (1 - self.update_rate) * tf.identity(self.lstm.Ui)\n",
        "        self.bi = self.update_rate * self.bi + (1 - self.update_rate) * tf.identity(self.lstm.bi)\n",
        "\n",
        "        self.Wf = self.update_rate * self.Wf + (1 - self.update_rate) * tf.identity(self.lstm.Wf)\n",
        "        self.Uf = self.update_rate * self.Uf + (1 - self.update_rate) * tf.identity(self.lstm.Uf)\n",
        "        self.bf = self.update_rate * self.bf + (1 - self.update_rate) * tf.identity(self.lstm.bf)\n",
        "\n",
        "        self.Wog = self.update_rate * self.Wog + (1 - self.update_rate) * tf.identity(self.lstm.Wog)\n",
        "        self.Uog = self.update_rate * self.Uog + (1 - self.update_rate) * tf.identity(self.lstm.Uog)\n",
        "        self.bog = self.update_rate * self.bog + (1 - self.update_rate) * tf.identity(self.lstm.bog)\n",
        "\n",
        "        self.Wc = self.update_rate * self.Wc + (1 - self.update_rate) * tf.identity(self.lstm.Wc)\n",
        "        self.Uc = self.update_rate * self.Uc + (1 - self.update_rate) * tf.identity(self.lstm.Uc)\n",
        "        self.bc = self.update_rate * self.bc + (1 - self.update_rate) * tf.identity(self.lstm.bc)\n",
        "\n",
        "        def unit(x, hidden_memory_tm1):\n",
        "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
        "\n",
        "            # Input Gate\n",
        "            i = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wi) +\n",
        "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
        "            )\n",
        "\n",
        "            # Forget Gate\n",
        "            f = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wf) +\n",
        "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
        "            )\n",
        "\n",
        "            # Output Gate\n",
        "            o = tf.sigmoid(\n",
        "                tf.matmul(x, self.Wog) +\n",
        "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
        "            )\n",
        "\n",
        "            # New Memory Cell\n",
        "            c_ = tf.nn.tanh(\n",
        "                tf.matmul(x, self.Wc) +\n",
        "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
        "            )\n",
        "\n",
        "            # Final Memory cell\n",
        "            c = f * c_prev + i * c_\n",
        "\n",
        "            # Current Hidden state\n",
        "            current_hidden_state = o * tf.nn.tanh(c)\n",
        "\n",
        "            return tf.stack([current_hidden_state, c])\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def create_output_unit(self):\n",
        "        self.Wo = tf.identity(self.lstm.Wo)\n",
        "        self.bo = tf.identity(self.lstm.bo)\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def update_output_unit(self):\n",
        "        self.Wo = self.update_rate * self.Wo + (1 - self.update_rate) * tf.identity(self.lstm.Wo)\n",
        "        self.bo = self.update_rate * self.bo + (1 - self.update_rate) * tf.identity(self.lstm.bo)\n",
        "\n",
        "        def unit(hidden_memory_tuple):\n",
        "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
        "            # hidden_state : batch x hidden_dim\n",
        "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
        "            # output = tf.nn.softmax(logits)\n",
        "            return logits\n",
        "\n",
        "        return unit\n",
        "\n",
        "    def update_params(self):\n",
        "        self.g_embeddings = tf.identity(self.lstm.g_embeddings)\n",
        "        self.g_recurrent_unit = self.update_recurrent_unit()\n",
        "        self.g_output_unit = self.update_output_unit()\n"
      ],
      "metadata": {
        "id": "s8k71OG6T7Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PXJOZl0dOBQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generator Hyperparameters\n",
        "#Hyperparameters\n",
        "\n",
        "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
        "SEQ_LENGTH = 20 # sequence length\n",
        "START_TOKEN = 0\n",
        "PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n",
        "SEED = 88\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zjPk3M2qPRiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Discriminator Hyperparameters\n",
        "\n",
        "dis_dropout_keep_prob = 0.75\n",
        "dis_l2_reg_lambda = 0.2\n",
        "dis_batch_size = 64\n"
      ],
      "metadata": {
        "id": "P2CVmOq5VYLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title General Hyperparameters and Directories\n",
        "\n",
        "TOTAL_BATCH = 200\n",
        "positive_file = 'save/real_data.txt'\n",
        "negative_file = 'save/generator_sample.txt'\n",
        "eval_file = 'save/eval_file.txt'\n",
        "generated_num = 10000\n"
      ],
      "metadata": {
        "id": "YnOXGduKVhrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main Model Code\n",
        "\n",
        "def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n",
        "  generated_samples = []\n",
        "  # dont forget the basics of ''join dont forget the basics of '' join, dont forget the basics of '' join\n",
        "\n",
        "  for _ in range(int(generated_num/batch_size)):\n",
        "    generated_samples.extend(trainable_model.generate(sess))\n",
        "\n",
        "  with open(output_file, 'w' ) as fout:\n",
        "    for sequence in generated_samples:\n",
        "      buffer = ','.join([str(x) for x in sequence]) + '\\n'\n",
        "      fout.write(buffer)\n",
        "\n",
        "# Code to test the distributional loss against the standard model.\n",
        "\n",
        "def target_loss(sess, target_lstm, data_loader):\n",
        "\n",
        "    nll = []\n",
        "    data_loader.reset_pointer()\n",
        "\n",
        "    for it in range(data_loader.num_batch):\n",
        "        batch = data_loader.next_batch()\n",
        "        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n",
        "        nll.append(g_loss)\n",
        "\n",
        "    return np.mean(nll)\n",
        "\n",
        "\n",
        "# Code for each pretraining epoch of the model\n",
        "\n",
        "def pre_train_epoch(sess, trainable_model, data_loader):\n",
        "  supervised_g_losses = []\n",
        "  data_loader.reset_pointer()\n",
        "  for it in range(data_loader.num_batch()):\n",
        "    batch = data_loader.next_batch()\n",
        "    _, g_loss = trainable_model.pretrain_step(sess,batch)\n",
        "    supervised_g_losses.append(g_loss)\n",
        "\n",
        "def main():\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  assert START_TOKEN == 0\n",
        "\n",
        "\n",
        "  gen_data_loader = gen_Data_loader(BATCH_SIZE)\n",
        "  likelihood_data_loader = Gen_Data_loader(BATCH_SIZE)\n",
        "  dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
        "  generator = Generator(1000, 50, 21,START_TOKEN)\n",
        "  ################ CODE FOR THE TARGET MODEL WE MUST BE ABLE TO GET\n",
        "  ################################################################\n",
        "  discriminator = Discriminator(sequence_length=20, num_classes=2,filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=0.4)\n",
        "\n",
        "  config = tf.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "  sess = tf.Session(config=config)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  #################################### Pre Training of the Generator\n",
        "  print(\"Pre Training Generator\")\n",
        "\n",
        "  log = open('save/experiment-log.txt', 'w')\n",
        "  for epoch in range(PRE_EPOCH_NUM):\n",
        "    loss = pre_train_epoch(sess, generator, gen_data_loader)\n",
        "    if epoch % 5 == 0:\n",
        "      generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
        "      likelihood_data_loader.create_batches(eval_file)\n",
        "      test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
        "      buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
        "      log.write(buffer)\n",
        "\n",
        "######################################## Pre Training of the Discriminator\n",
        "  print(\"Pre Training Discriminator\")\n",
        "\n",
        "  for _ in range(50):\n",
        "    generate_samples(sess,generator, BATCH_SIZE, generated_num, negative_file)\n",
        "    dis_data_loader.load_train_data(positve_file, negative_file)\n",
        "\n",
        "    for _ in range(3):\n",
        "      dis_data_loader.reset_pointer()\n",
        "      for it in range(dis_data_loader.num_batch):\n",
        "        x_batch, y_batch = dis_data_loader.next_batch()\n",
        "        feed = {\n",
        "            discriminator.input_x: x_batch,\n",
        "            discriminator.input_y: y_batch,\n",
        "            discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
        "\n",
        "        }\n",
        "        _ = sess.run(discriminator.train_op, feed)\n",
        "\n",
        "    rollout = ROLLOUT(generator, 0.8)\n",
        "################################################################# Adversarial Training\n",
        "  log.write('adversarial training...\\n')\n",
        "\n",
        "  for total_batch in range(TOTAL_BATCH):\n",
        "    for it in range(1):\n",
        "      samples = generator.generate(sess)\n",
        "      rewards = rollout.get_reward(sess, samples, 16, discriminator)\n",
        "      feed = {generator.x: samples, generator.rewards: rewards}\n",
        "      _ = sess.run(generator.g_updates, feed_dict=feed)\n",
        "\n",
        "    if total_batch % 5 == 0 or total_batch == TOTAL_BATCH -1:\n",
        "      generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
        "      likelihood_data_loader.create_batches(eval_file)\n",
        "      test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
        "      buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
        "      print ('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
        "      log.write(buffer)\n",
        "\n",
        "\n",
        "    rollout.update_params()\n",
        "\n",
        "    for _ in range(5):\n",
        "      generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n",
        "      dis_data_loader.load_train_data(positive_file, negative_file)\n",
        "\n",
        "      for _ in range(3):\n",
        "        dis_data_loader.reset_pointer()\n",
        "        for it in range(dis_data_loader.num_batch):\n",
        "          x_batch, y_batch = dis_data_loader.next_batch()\n",
        "\n",
        "          feed = {\n",
        "              discriminator.input_x: x_batch,\n",
        "              discriminator.input_y: y_batch,\n",
        "              discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
        "\n",
        "          }\n",
        "          _ = sess.run(discriminator.train_op, feed)\n",
        "\n",
        "    log.close()\n",
        "\n",
        "\n",
        "#    if __name__ == '__main__':\n",
        "#      main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YzHCb6NMULIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDrUdGlitHO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}