{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JXRz10YwNjcq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Additional PyTorch-specific imports if needed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generator Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY1O0ADQQS06"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class gen_Data_loader():\n",
        "  def __init__(self,batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.token_stream = []\n",
        "\n",
        "  def create_batches(self,data_file):\n",
        "    self.token_stream = []\n",
        "    with open(data_file, \"r\") as f:\n",
        "      for line in f:\n",
        "        line = line.strip()\n",
        "        line = line.split()\n",
        "        parse_line = [int(x) for x in line]\n",
        "        if len(parse_line) == 20:\n",
        "\n",
        "          self.token_stream.append(parse_line)\n",
        "    self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
        "    self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
        "    self.sequence_batch = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
        "    self.pointer = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    ret = self.sequence_batch[self.pointer]\n",
        "    self.pointer = (self.pointer +1) % self.num_batch\n",
        "    return ret\n",
        "  def reset_pointer(self):\n",
        "    self.pointer = 0\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discriminator Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob7LZiPPXJS1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class Dis_dataloader():\n",
        "  def __init__(self, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.sentences = np.array([])\n",
        "    self.labels = np.array([])\n",
        "\n",
        "  def load_train_data(self, positive_file, negative_file):\n",
        "    positive_examples = []\n",
        "    negative_examples = []\n",
        "    with open(positive_file) as fin:\n",
        "      for line in fin:\n",
        "          line = line.strip()\n",
        "          line = line.split()\n",
        "          parse_line = [int(x) for x in line]\n",
        "          positive_examples.append(parse_line)\n",
        "    with open(negative_file)as fin:\n",
        "      for line in fin:\n",
        "          line = line.strip()\n",
        "          line = line.split()\n",
        "          parse_line = [int(x) for x in line]\n",
        "          if len(parse_line) == 20:\n",
        "            negative_examples.append(parse_line)\n",
        "          self.sentences = np.array(positive_examples + negative_examples)\n",
        "          positive_labels = [[0, 1] for _ in positive_examples]\n",
        "          negative_labels = [[1, 0] for _ in negative_examples]\n",
        "          self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
        "\n",
        "          # Shuffle the data\n",
        "          shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
        "          self.sentences = self.sentences[shuffle_indices]\n",
        "          self.labels = self.labels[shuffle_indices]\n",
        "\n",
        "          # Split batches\n",
        "          self.num_batch = int(len(self.labels) / self.batch_size)\n",
        "          self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
        "          self.labels = self.labels[:self.num_batch * self.batch_size]\n",
        "          self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
        "          self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
        "\n",
        "          self.pointer = 0\n",
        "  def next_batch(self):\n",
        "      ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
        "      self.pointer = (self.pointer + 1) % self.num_batch\n",
        "      return ret\n",
        "\n",
        "  def reset_pointer(self):\n",
        "      self.pointer = 0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discriminator Code "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MARQoXUnc6rh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Highway(nn.Module):\n",
        "    def __init__(self, size, num_layers=1, bias=-2.0, activation=F.relu):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        # one linear and one gate per layer\n",
        "        self.linears = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "        self.gates   = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, size]\n",
        "        for lin, gate in zip(self.linears, self.gates):\n",
        "            g = self.activation(lin(x))\n",
        "            t = torch.sigmoid(gate(x) + self.bias)\n",
        "            x = t * g + (1 - t) * x\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "#Coding the actual discriminator itself\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, sequence_length, data_size, l2_reg_lambda=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sequence_length: int, length of your input “sequence” dimension\n",
        "          data_size:        int, the number of channels/features per step\n",
        "          l2_reg_lambda:    float, coefficient for L2 penalty (pass as weight_decay to your optimizer)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.l2_reg_lambda = l2_reg_lambda\n",
        "\n",
        "        # two 2D conv layers (assumes input x shaped [B, 1, L, D])\n",
        "        self.conv1 = nn.Conv2d(1,  64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
        "\n",
        "        # compute flattened size after conv2\n",
        "        flat_size = 32 * sequence_length * data_size\n",
        "\n",
        "        # highway block\n",
        "        self.highway = Highway(flat_size, num_layers=2)\n",
        "\n",
        "        # six fully-connected layers\n",
        "        self.fc1 = nn.Linear(flat_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024,     512)\n",
        "        self.fc3 = nn.Linear(512,      256)\n",
        "        self.fc4 = nn.Linear(256,      128)\n",
        "        self.fc5 = nn.Linear(128,       64)\n",
        "        self.fc6 = nn.Linear(64,        32)\n",
        "\n",
        "        # a 1×1 conv + pool on final FC output\n",
        "        self.conv_pool_conv = nn.Conv2d(32, 64, kernel_size=1)\n",
        "        self.pool            = nn.MaxPool2d(kernel_size=1)\n",
        "\n",
        "        # final score layer\n",
        "        self.output = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x, dropout_keep_prob=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x:                  Tensor of shape [batch, 1, sequence_length, data_size]\n",
        "          dropout_keep_prob:  float in (0,1], the keep probability\n",
        "        Returns:\n",
        "          scores:      [batch, 2]\n",
        "          predictions: [batch] long tensor of predicted class\n",
        "        \"\"\"\n",
        "        # conv stack\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # highway + dropout\n",
        "        x = self.highway(x)\n",
        "        x = F.dropout(x, p=1-dropout_keep_prob, training=self.training)\n",
        "\n",
        "        # six FC + dropout blocks\n",
        "        for fc in (self.fc1, self.fc2, self.fc3, self.fc4, self.fc5, self.fc6):\n",
        "            x = F.relu(fc(x))\n",
        "            x = F.dropout(x, p=1-dropout_keep_prob, training=self.training)\n",
        "\n",
        "        # reshape for conv-pool\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)    # → [B, 32, 1, 1]\n",
        "        x = F.relu(self.conv_pool_conv(x))   # → [B, 64, 1, 1]\n",
        "        x = self.pool(x)                     # → [B, 64, 1, 1]\n",
        "        x = x.view(x.size(0), -1)            # → [B, 64]\n",
        "\n",
        "        # final scores & preds\n",
        "        scores      = self.output(x)         # [B, 2]\n",
        "        predictions = torch.argmax(scores, dim=1)\n",
        "\n",
        "        return scores, predictions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju38NlP4Mg6b"
      },
      "source": [
        "# Generator Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbs_1N515zxY"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_dim: int,\n",
        "                 seq_len: int,\n",
        "                 start_value: float,\n",
        "                 reward_gamma: float = 0.95,\n",
        "                 temperature: float = 1.0,\n",
        "                 grad_clip: float = 5.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_dim:    hidden size of the LSTM cell\n",
        "            seq_len:       length of sequence to generate\n",
        "            start_value:   initial value to feed in at t=0 when sampling\n",
        "            reward_gamma:  discount factor (if you’ll implement RL later)\n",
        "            temperature:   scaling factor for sampling\n",
        "            grad_clip:     max‐norm for gradient clipping\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_dim   = hidden_dim\n",
        "        self.seq_len      = seq_len\n",
        "        self.start_value  = start_value\n",
        "        self.reward_gamma = reward_gamma\n",
        "        self.temperature  = temperature\n",
        "        self.grad_clip    = grad_clip\n",
        "\n",
        "        # your “recurrent unit” → LSTMCell\n",
        "        self.lstm_cell = nn.LSTMCell(1, hidden_dim)\n",
        "\n",
        "        # your “output unit” → project hidden to numerical values\n",
        "        self.output_linear = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Teacher-forcing pass for pretraining.\n",
        "        Args:\n",
        "            x: [batch, seq_len, 1] tensor of numerical values\n",
        "        Returns:\n",
        "            outputs: [batch, seq_len, 1]\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        device     = x.device\n",
        "\n",
        "        # initialize (h,c) to zeros\n",
        "        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n",
        "        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n",
        "\n",
        "        outputs_time = []\n",
        "        for t in range(self.seq_len):\n",
        "            h, c = self.lstm_cell(x[:, t, :], (h, c))\n",
        "            output = self.output_linear(h).unsqueeze(1)  # [B, 1]\n",
        "            outputs_time.append(output)\n",
        "\n",
        "        # [B, T, 1]\n",
        "        outputs = torch.cat(outputs_time, dim=1)\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, batch_size: int = None):\n",
        "        \"\"\"\n",
        "        Sequential sampling (for adversarial roll-outs).\n",
        "        Args:\n",
        "            batch_size: if None, uses the batch_size given in forward calls\n",
        "        Returns:\n",
        "            samples: [batch_size, seq_len, 1] tensor of sampled numerical values\n",
        "        \"\"\"\n",
        "        if batch_size is None:\n",
        "            raise ValueError(\"Please pass batch_size when sampling\")\n",
        "\n",
        "        device = next(self.parameters()).device\n",
        "        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n",
        "        c = torch.zeros(batch_size, self.hidden_dim, device=device)\n",
        "\n",
        "        # start with the start value\n",
        "        inp = torch.full((batch_size, 1), self.start_value, dtype=torch.float, device=device)\n",
        "        samples = []\n",
        "\n",
        "        for _ in range(self.seq_len):\n",
        "            h, c = self.lstm_cell(inp, (h, c))    # [B, H]\n",
        "            output = self.output_linear(h) / self.temperature\n",
        "            inp = output                          # feed output as next input\n",
        "            samples.append(output.unsqueeze(1))\n",
        "\n",
        "        # [B, T, 1]\n",
        "        return torch.cat(samples, dim=1)\n",
        "\n",
        "    def pretrain_loss(self, outputs: torch.Tensor, target: torch.Tensor):\n",
        "        \"\"\"\n",
        "        MSE loss for numerical values.\n",
        "        Args:\n",
        "            outputs: [B, T, 1]\n",
        "            target: [B, T, 1]\n",
        "        \"\"\"\n",
        "        mse = F.mse_loss(outputs, target, reduction='mean')\n",
        "        return mse\n",
        "    def pretrain_step(self, optimizer, outputs: torch.Tensor, target: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Perform a pretraining step with gradient clipping.\n",
        "        Args:\n",
        "            optimizer: PyTorch optimizer for the generator.\n",
        "            outputs: [B, T, 1] tensor of generated numerical values.\n",
        "            target: [B, T, 1] tensor of target numerical values.\n",
        "        Returns:\n",
        "            loss: Scalar loss value.\n",
        "        \"\"\"\n",
        "        # Compute the MSE loss\n",
        "        loss = F.mse_loss(outputs, target, reduction='mean')\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rollout Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8k71OG6T7Wt"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Rollout(nn.Module):\n",
        "    def __init__(self, generator, update_rate):\n",
        "        \"\"\"\n",
        "        generator: Generator model with LSTM layer\n",
        "        update_rate: float in (0,1), how fast rollout net tracks generator\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.update_rate = update_rate\n",
        "        self.sequence_length = generator.sequence_length\n",
        "        self.device = next(generator.parameters()).device\n",
        "        \n",
        "        # Create LSTM and linear layers matching generator's architecture\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=generator.lstm.input_size,\n",
        "            hidden_size=generator.lstm.hidden_size,\n",
        "            num_layers=generator.lstm.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.output_layer = nn.Linear(\n",
        "            generator.lstm.hidden_size,\n",
        "            generator.output_layer.out_features\n",
        "        )\n",
        "        \n",
        "        # Initialize with generator's weights\n",
        "        self.update_params()\n",
        "\n",
        "    def _rollout(self, prefix, given_len):\n",
        "        \"\"\"Generate continuation of the prefix sequence\"\"\"\n",
        "        batch_size = prefix.size(0)\n",
        "        \n",
        "        # Process prefix through LSTM\n",
        "        hidden = None\n",
        "        output, hidden = self.lstm(prefix[:, :given_len, :], hidden)\n",
        "        \n",
        "        # Generate remaining sequence\n",
        "        current = prefix[:, given_len-1:given_len, :]\n",
        "        samples = [prefix[:, :given_len, :]]\n",
        "        \n",
        "        for t in range(given_len, self.sequence_length):\n",
        "            output, hidden = self.lstm(current, hidden)\n",
        "            next_value = self.output_layer(output)\n",
        "            samples.append(next_value)\n",
        "            current = next_value\n",
        "            \n",
        "        return torch.cat(samples, dim=1)\n",
        "\n",
        "    def get_reward(self, input_x, rollout_num, discriminator):\n",
        "        \"\"\"Calculate rewards for each position in the sequence\"\"\"\n",
        "        batch_size = input_x.size(0)\n",
        "        rewards = []\n",
        "        \n",
        "        for n in range(rollout_num):\n",
        "            for given_len in range(1, self.sequence_length):\n",
        "                samples = self._rollout(input_x, given_len)\n",
        "                with torch.no_grad():\n",
        "                    scores = discriminator(samples)\n",
        "                    prob = torch.sigmoid(scores)\n",
        "                if n == 0:\n",
        "                    rewards.append(prob.cpu().numpy())\n",
        "                else:\n",
        "                    rewards[given_len-1] += prob.cpu().numpy()\n",
        "\n",
        "            # Reward for complete sequence\n",
        "            with torch.no_grad():\n",
        "                scores = discriminator(input_x)\n",
        "                prob = torch.sigmoid(scores)\n",
        "            if n == 0:\n",
        "                rewards.append(prob.cpu().numpy())\n",
        "            else:\n",
        "                rewards[self.sequence_length-1] += prob.cpu().numpy()\n",
        "\n",
        "        rewards = np.array(rewards).T / float(rollout_num)\n",
        "        return rewards\n",
        "\n",
        "    def update_params(self):\n",
        "        \"\"\"Update rollout network weights based on generator\"\"\"\n",
        "        with torch.no_grad():\n",
        "            for rollout_param, gen_param in zip(self.lstm.parameters(), \n",
        "                                              self.generator.lstm.parameters()):\n",
        "                rollout_param.data.copy_(\n",
        "                    self.update_rate * rollout_param.data + \n",
        "                    (1 - self.update_rate) * gen_param.data\n",
        "                )\n",
        "            \n",
        "            for rollout_param, gen_param in zip(self.output_layer.parameters(), \n",
        "                                              self.generator.output_layer.parameters()):\n",
        "                rollout_param.data.copy_(\n",
        "                    self.update_rate * rollout_param.data + \n",
        "                    (1 - self.update_rate) * gen_param.data\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXJOZl0dOBQu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generator Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjPk3M2qPRiY"
      },
      "outputs": [],
      "source": [
        "\n",
        "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
        "SEQ_LENGTH = 20 # sequence length\n",
        "START_TOKEN = 0\n",
        "PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n",
        "SEED = 88\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discriminator Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2CVmOq5VYLc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dis_dropout_keep_prob = 0.75\n",
        "dis_l2_reg_lambda = 0.2\n",
        "dis_batch_size = 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# General Hyperparameters and Directories "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnOXGduKVhrB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "TOTAL_BATCH = 200\n",
        "positive_file = 'save/real_data.txt'\n",
        "negative_file = 'save/generator_sample.txt'\n",
        "eval_file = 'save/eval_file.txt'\n",
        "generated_num = 10000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main Model Code "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "YzHCb6NMULIs",
        "outputId": "1ae8e568-d418-4c1d-aa7b-6d945cd164fb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generate_samples(model, batch_size, generated_num, output_file):\n",
        "  model.eval()\n",
        "  generated_samples = []\n",
        "\n",
        "  for _ in range(int(generated_num / batch_size)):\n",
        "    samples = model.generate(batch_size)\n",
        "    generated_samples.extend(samples.cpu().numpy())\n",
        "\n",
        "  with open(output_file, 'w') as fout:\n",
        "    for sequence in generated_samples:\n",
        "      buffer = ','.join([str(x) for x in sequence]) + '\\n'\n",
        "      fout.write(buffer)\n",
        "\n",
        "def target_loss(model, data_loader, criterion):\n",
        "  model.eval()\n",
        "  nll = []\n",
        "\n",
        "  for batch in data_loader:\n",
        "    batch = batch.to(device)\n",
        "    g_loss = criterion(model(batch), batch)\n",
        "    nll.append(g_loss.item())\n",
        "\n",
        "  return np.mean(nll)\n",
        "\n",
        "def pre_train_epoch(model, data_loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  supervised_g_losses = []\n",
        "\n",
        "  for batch in data_loader:\n",
        "    batch = batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(batch)\n",
        "    g_loss = criterion(output, batch)\n",
        "    g_loss.backward()\n",
        "    optimizer.step()\n",
        "    supervised_g_losses.append(g_loss.item())\n",
        "\n",
        "  return np.mean(supervised_g_losses)\n",
        "\n",
        "\n",
        "def main():\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "\n",
        "  gen_data_loader = gen_Data_loader(BATCH_SIZE)\n",
        "  likelihood_data_loader = gen_Data_loader(BATCH_SIZE)\n",
        "  dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
        "\n",
        "  generator = Generator(vocab_size=1000, embedding_dim=50, hidden_dim=21, sequence_length=21, start_token=START_TOKEN).to(device)\n",
        "  discriminator = Discriminator(sequence_length=20, num_classes=2, embedding_dim=50, hidden_dim=64, dropout_prob=0.4).to(device)\n",
        "\n",
        "  gen_optimizer = optim.Adam(generator.parameters(), lr=1e-3)\n",
        "  dis_optimizer = optim.Adam(discriminator.parameters(), lr=1e-3)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Pre-training Generator\n",
        "  print(\"Pre Training Generator\")\n",
        "  for epoch in range(PRE_EPOCH_NUM):\n",
        "    loss = pre_train_epoch(generator, gen_data_loader, gen_optimizer, criterion)\n",
        "    if epoch % 5 == 0:\n",
        "      generate_samples(generator, BATCH_SIZE, generated_num, eval_file)\n",
        "      likelihood_data_loader.create_batches(eval_file)\n",
        "      test_loss = target_loss(generator, likelihood_data_loader, criterion)\n",
        "      print(f\"Epoch: {epoch}, NLL: {test_loss}\")\n",
        "\n",
        "  # Pre-training Discriminator\n",
        "  print(\"Pre Training Discriminator\")\n",
        "  for _ in range(50):\n",
        "    generate_samples(generator, BATCH_SIZE, generated_num, negative_file)\n",
        "    dis_data_loader.load_train_data(positive_file, negative_file)\n",
        "\n",
        "    for _ in range(3):\n",
        "      for x_batch, y_batch in dis_data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        dis_optimizer.zero_grad()\n",
        "        logits = discriminator(x_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "  # Adversarial Training\n",
        "  rollout = Rollout(generator, 0.8)\n",
        "  for total_batch in range(TOTAL_BATCH):\n",
        "    for _ in range(1):\n",
        "      samples = generator.generate(BATCH_SIZE)\n",
        "      rewards = rollout.get_reward(samples, discriminator)\n",
        "      gen_optimizer.zero_grad()\n",
        "      loss = -torch.mean(rewards)\n",
        "      loss.backward()\n",
        "      gen_optimizer.step()\n",
        "\n",
        "    rollout.update_params()\n",
        "\n",
        "    for _ in range(5):\n",
        "      generate_samples(generator, BATCH_SIZE, generated_num, negative_file)\n",
        "      dis_data_loader.load_train_data(positive_file, negative_file)\n",
        "\n",
        "      for _ in range(3):\n",
        "        for x_batch, y_batch in dis_data_loader:\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          dis_optimizer.zero_grad()\n",
        "          logits = discriminator(x_batch)\n",
        "          loss = criterion(logits, y_batch)\n",
        "          loss.backward()\n",
        "          dis_optimizer.step()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDrUdGlitHO0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
